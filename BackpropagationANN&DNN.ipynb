{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackpropagationANN&DNN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlp5TbUXU1crg7ldz3o3N5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinhVuong2000/Data-Science/blob/master/BackpropagationANN%26DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iztqtD4-5T3-"
      },
      "source": [
        "#### Good blog\n",
        "1. https://missinglink.ai/guides/neural-network-concepts/backpropagation-neural-networks-process-examples-code-minus-math/#:~:text=Backpropagation%20is%20an%20algorithm%20commonly,one%2C%20given%20the%20initial%20weights.\n",
        "2. https://www.linkedin.com/pulse/forward-back-propagation-over-cnn-code-from-scratch-coy-ulloa/\n",
        "3. https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\n",
        "4. https://datascience.stackexchange.com/questions/27506/back-propagation-in-cnn\n",
        "5. https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXURJ-bWPF_7"
      },
      "source": [
        "def conv_forward(X, W):\n",
        "    '''\n",
        "    The forward computation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    X -- output activations of the previous layer, numpy array of shape (n_H_prev, n_W_prev) assuming input channels = 1\n",
        "    W -- Weights, numpy array of size (f, f) assuming number of filters = 1\n",
        "    \n",
        "    Returns:\n",
        "    H -- conv output, numpy array of size (n_H, n_W)\n",
        "    cache -- cache of values needed for conv_backward() function\n",
        "    '''\n",
        "    \n",
        "    # Retrieving dimensions from X's shape\n",
        "    (n_H_prev, n_W_prev) = X.shape\n",
        "    \n",
        "    # Retrieving dimensions from W's shape\n",
        "    (f, f) = W.shape\n",
        "    \n",
        "    # Compute the output dimensions assuming no padding and stride = 1\n",
        "    n_H = n_H_prev - f + 1\n",
        "    n_W = n_W_prev - f + 1\n",
        "    \n",
        "    # Initialize the output H with zeros\n",
        "    H = np.zeros((n_H, n_W))\n",
        "    \n",
        "    # Looping over vertical(h) and horizontal(w) axis of output volume\n",
        "    for h in range(n_H):\n",
        "        for w in range(n_W):\n",
        "            x_slice = X[h:h+f, w:w+f]\n",
        "            H[h,w] = np.sum(x_slice * W)\n",
        "            \n",
        "    # Saving information in 'cache' for backprop\n",
        "    cache = (X, W)\n",
        "    \n",
        "    return H, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua4ebvd4PWdx"
      },
      "source": [
        "def conv_backward(dH, cache):\n",
        "    '''\n",
        "    The backward computation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
        "    '''\n",
        "    \n",
        "    # Retrieving information from the \"cache\"\n",
        "    (X, W) = cache\n",
        "    \n",
        "    # Retrieving dimensions from X's shape\n",
        "    (n_H_prev, n_W_prev) = X.shape\n",
        "    \n",
        "    # Retrieving dimensions from W's shape\n",
        "    (f, f) = W.shape\n",
        "    \n",
        "    # Retrieving dimensions from dH's shape\n",
        "    (n_H, n_W) = dH.shape\n",
        "    \n",
        "    # Initializing dX, dW with the correct shapes\n",
        "    dX = np.zeros(X.shape)\n",
        "    dW = np.zeros(W.shape)\n",
        "    \n",
        "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
        "    for h in range(n_H):\n",
        "        for w in range(n_W):\n",
        "            dX[h:h+f, w:w+f] += W * dH(h,w)\n",
        "            dW += X[h:h+f, w:w+f] * dH(h,w)\n",
        "    \n",
        "    return dX, dW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uezzI4JMP7Xj"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y0 = tf.constant( y_ , dtype=tf.float32 )\n",
        "\n",
        "# Layer 1 = the 2x3 hidden sigmoid\n",
        "m1 = tf.Variable( tf.random_uniform( [2,3] , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "b1 = tf.Variable( tf.random_uniform( [3]   , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "h1 = tf.sigmoid( tf.matmul( x0,m1 ) + b1 )\n",
        "\n",
        "# Layer 2 = the 3x1 sigmoid output\n",
        "m2 = tf.Variable( tf.random_uniform( [3,1] , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "b2 = tf.Variable( tf.random_uniform( [1]   , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "y_out = tf.sigmoid( tf.matmul( h1,m2 ) + b2 )\n",
        "\n",
        "\n",
        "### loss\n",
        "# loss : sum of the squares of y0—y_out\n",
        "loss = tf.reduce_sum( tf.square( y0—y_out ) )\n",
        "\n",
        "# training step : gradient descent (1.0) to minimize loss\n",
        "train = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
        "\n",
        "\n",
        "### training\n",
        "# run 500 times using all the X and Y\n",
        "# print out the loss and any other interesting info\n",
        "with tf.Session() as sess:\n",
        "  sess.run( tf.global_variables_initializer() )\n",
        "  for step in range(500) :\n",
        "    sess.run(train)\n",
        "\n",
        "  results = sess.run([m1,b1,m2,b2,y_out,loss])\n",
        "  labels  = \"m1,b1,m2,b2,y_out,loss\".split(\",\")\n",
        "  for label,result in zip(*(labels,results)) :\n",
        "    print \"\"\n",
        "    print label\n",
        "    print result\n",
        "\n",
        "print \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vrw8TpEwnW"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\t\"\"\"Convolutional Neural Networks\"\"\"\n",
        "\t\n",
        "    import numpy as np\n",
        "\t\n",
        "\n",
        "\t\n",
        "\n",
        "\tdef conv_forward(A_prev, W, b, activation, padding=\"same\", stride=(1, 1)):\n",
        "\t    \"\"\"forward prop convolutional 3D image, RGB image - color\n",
        "\t    \n",
        "        Arg:\n",
        "\t       A_prev: contains the output of prev layer (m, h_prev, w_prev, c_prev)\n",
        "\t       W: filter for the convolution (kh, kw, c_prev, c_new)\n",
        "\t       b: biases (1, 1, 1, c_new)\n",
        "\t       padding: string ‘same’, or ‘valid’\n",
        "\t       stride: tuple (sh, sw)\n",
        "\t    \n",
        "        Return: padded convolved images RGB np.array\n",
        "\t    \"\"\"\n",
        "\n",
        "\t    m, h_prev, w_prev, c_prev = A_prev.shape\n",
        "\t    k_h, k_w, c_prev, c_new = W.shape\n",
        "\t    s_h, s_w = stride\n",
        "\t\n",
        "\n",
        "\t    if padding == 'valid':\n",
        "\t        p_h = 0\n",
        "\t        p_w = 0\n",
        "\t\n",
        "\n",
        "\t    if padding == 'same':\n",
        "\t        p_h = np.ceil(((s_h*h_prev) - s_h + k_h - h_prev) / 2)\n",
        "\t        p_h = int(p_h)\n",
        "\t        p_w = np.ceil(((s_w*w_prev) - s_w + k_w - w_prev) / 2)\n",
        "\t        p_w = int(p_w)\n",
        "\t\n",
        "\n",
        "\t    A_prev = np.pad(A_prev, [(0, 0), (p_h, p_h), (p_w, p_w), (0, 0)],\n",
        "\t                    mode='constant', constant_values=0)\n",
        "\t\n",
        "\n",
        "\t    out_h = int(((h_prev - k_h + (2*p_h)) / (stride[0])) + 1)\n",
        "\t    out_w = int(((w_prev - k_w + (2*p_w)) / (stride[1])) + 1)\n",
        "\t    output_conv = np.zeros((m, out_h, out_w, c_new))\n",
        "\t    m_A_prev = np.arange(0, m)\n",
        "\t\n",
        "\n",
        "\t    for i in range(out_h):\n",
        "\t        for j in range(out_w):\n",
        "\t            for f in range(c_new):\n",
        "\t                output_conv[m_A_prev, i, j, f] = activation((\n",
        "\t                    np.sum(np.multiply(\n",
        "\t                        A_prev[\n",
        "\t                            m_A_prev,\n",
        "\t                            i*(stride[0]):k_h+(i*(stride[0])),\n",
        "\t                            j*(stride[1]):k_w+(j*(stride[1]))],\n",
        "\t                        W[:, :, :, f]), axis=(1, 2, 3))) + b[0, 0, 0, f])\n",
        "\t    \n",
        "        \n",
        "        return output_conv\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\t    np.random.seed(0)\n",
        "\t    lib = np.load('../data/MNIST.npz')\n",
        "\t    X_train = lib['X_train']\n",
        "\t    m, h, w = X_train.shape\n",
        "\t    X_train_c = X_train.reshape((-1, h, w, 1))\n",
        "\t\n",
        "\n",
        "\t    W = np.random.randn(3, 3, 1, 2)\n",
        "\t    b = np.random.randn(1, 1, 1, 2)\n",
        "\t\n",
        "\n",
        "\t    def relu(Z):\n",
        "\t        return np.maximum(Z, 0)\n",
        "\t\n",
        "\n",
        "\t    plt.imshow(X_train[0])\n",
        "\t    plt.show()\n",
        "\t    A = conv_forward(X_train_c, W, b, relu, padding='valid')\n",
        "\t    print(A.shape)\n",
        "\t    plt.imshow(A[0, :, :, 0])\n",
        "\t    plt.show()\n",
        "\t    plt.imshow(A[0, :, :, 1])\n",
        "\t    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5rhLHp-E0Dd"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\t\"\"\"Convolutional Neural Networks\"\"\"\n",
        "\timport numpy as np\n",
        "\t\n",
        "\n",
        "\t\n",
        "\n",
        "\tdef conv_backward(dZ, A_prev, W, b, padding=\"same\", stride=(1, 1)):\n",
        "\t    \"\"\"back prop convolutional 3D image, RGB image - color\n",
        "\t    Arg:\n",
        "\t       dZ: containing the partial derivatives (m, h_new, w_new, c_new)\n",
        "\t       A_prev: contains the output of prev layer (m, h_prev, w_prev, c_prev)\n",
        "\t       W: filter for the convolution (kh, kw, c_prev, c_new)\n",
        "\t       b: biases (1, 1, 1, c_new)\n",
        "\t       padding: string ‘same’, or ‘valid’\n",
        "\t       stride: tuple (sh, sw)\n",
        "\t    Returns: parcial dev prev layer (dA_prev), kernels (dW), biases (db)\n",
        "\t    \"\"\"\n",
        "\t    k_h, k_w, c_prev, c_new = W.shape\n",
        "\t    _, h_new, w_new, c_new = dZ.shape\n",
        "\t    m, h_x, w_x, c_prev = A_prev.shape\n",
        "\t    s_h, s_w = stride\n",
        "\t    x = A_prev\n",
        "\t\n",
        "\n",
        "\t    if padding == 'valid':\n",
        "\t        p_h = 0\n",
        "\t        p_w = 0\n",
        "\t\n",
        "\n",
        "\t    if padding == 'same':\n",
        "\t        p_h = np.ceil(((s_h*h_x) - s_h + k_h - h_x) / 2)\n",
        "\t        p_h = int(p_h)\n",
        "\t        p_w = np.ceil(((s_w*w_x) - s_w + k_w - w_x) / 2)\n",
        "\t        p_w = int(p_w)\n",
        "\t\n",
        "\n",
        "\t    db = np.sum(dZ, axis=(0, 1, 2), keepdims=True)\n",
        "\t\n",
        "\n",
        "\t    x_padded = np.pad(x, [(0, 0), (p_h, p_h), (p_w, p_w), (0, 0)],\n",
        "\t                      mode='constant', constant_values=0)\n",
        "\t\n",
        "\n",
        "\t    dW = np.zeros_like(W)\n",
        "\t    dx = np.zeros(x_padded.shape)\n",
        "\t    m_i = np.arange(m)\n",
        "\t    for i in range(m):\n",
        "\t        for h in range(h_new):\n",
        "\t            for w in range(w_new):\n",
        "\t                for f in range(c_new):\n",
        "\t                    dx[i,\n",
        "\t                       h*(stride[0]):(h*(stride[0]))+k_h,\n",
        "\t                       w*(stride[1]):(w*(stride[1]))+k_w,\n",
        "\t                       :] += dZ[i, h, w, f] * W[:, :, :, f]\n",
        "\t\n",
        "\n",
        "\t                    dW[:, :,\n",
        "\t                       :, f] += x_padded[i,\n",
        "\t                                         h*(stride[0]):(h*(stride[0]))+k_h,\n",
        "\t                                         w*(stride[1]):(w*(stride[1]))+k_w,\n",
        "\t                                         :] * dZ[i, h, w, f]\n",
        "\t    if padding == 'same':\n",
        "\t        dx = dx[:, p_h:-p_h, p_w:-p_w, :]\n",
        "\t    else:\n",
        "\t        dx = dx\n",
        "\t\n",
        "\n",
        "\t    return dx, dW, db\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(0)\n",
        "    lib = np.load('../data/MNIST.npz')\n",
        "    X_train = lib['X_train']\n",
        "    _, h, w = X_train.shape\n",
        "    X_train_c = X_train[:10].reshape((-1, h, w, 1))\n",
        "\n",
        "    W = np.random.randn(3, 3, 1, 2)\n",
        "    b = np.random.randn(1, 1, 1, 2)\n",
        "\n",
        "    dZ = np.random.randn(10, h - 2, w - 2, 2)\n",
        "    print(conv_backward(dZ, X_train_c, W, b, padding=\"valid\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}